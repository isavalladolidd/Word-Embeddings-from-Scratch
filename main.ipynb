{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc3c728",
   "metadata": {},
   "source": [
    "## Skip-gram vs CBOW – Word Embeddings from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a8fc9",
   "metadata": {},
   "source": [
    "In this assignment, students will:\n",
    "\n",
    "- Implement Skip-gram and CBOW models from scratch using PyTorch.\n",
    "- Train these models on a real-world text corpus.\n",
    "- Visualize and compare the learned word embeddings using t-SNE and UMAP.\n",
    "- Interpret the semantic structure of the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6149134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32927662",
   "metadata": {},
   "source": [
    "### Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d82baa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del texto: 90000000\n",
      "Primeros 500 caracteres:\n",
      "  anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n"
     ]
    }
   ],
   "source": [
    "# cargar datos desde Hugging Face\n",
    "ds = load_dataset(\"afmck/text8\")\n",
    "\n",
    "# extraer texto\n",
    "text = ds['train'][0]['text']\n",
    "\n",
    "# Confirmar contenido\n",
    "print(\"Longitud del texto:\", len(text))\n",
    "print(\"Primeros 500 caracteres:\\n\", text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b3c3e",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e7cdc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50000\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "tokens = text.split()\n",
    "\n",
    "freq = collections.Counter(tokens)\n",
    "tokens = [w for w in tokens if freq[w] >= 5]\n",
    "\n",
    "most_common = [w for w, _ in freq.most_common(50000)]\n",
    "vocab = set(most_common)\n",
    "tokens = [w for w in tokens if w in vocab]\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(most_common)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68dfd4c",
   "metadata": {},
   "source": [
    "### Generar pares de entrenamiento (CBOW y Skip-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d64779a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating skipgram pairs: 100%|██████████| 14928870/14928870 [00:18<00:00, 786091.31it/s]\n",
      "generating cbow pairs: 100%|██████████| 14928870/14928870 [01:54<00:00, 130533.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip pairs: 104505627\n",
      "CBOW pairs: 14928870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_pairs(tokens, word2idx, window_min=2, window_max=5, mode='skipgram'):\n",
    "    pairs = []\n",
    "    T = len(tokens)\n",
    "    for i in tqdm(range(T), desc=f'generating {mode} pairs'):\n",
    "        if tokens[i] not in word2idx:\n",
    "            continue\n",
    "        center = word2idx[tokens[i]]\n",
    "        window = random.randint(window_min, window_max)\n",
    "        start = max(0, i - window)\n",
    "        end = min(T, i + window + 1)\n",
    "        context = []\n",
    "        for j in range(start, end):\n",
    "            if j == i: continue\n",
    "            if tokens[j] not in word2idx: continue\n",
    "            context.append(word2idx[tokens[j]])\n",
    "        if len(context) == 0:\n",
    "            continue\n",
    "        if mode == 'skipgram':\n",
    "            # emit (center, each_context) as separate samples\n",
    "            for ctx in context:\n",
    "                pairs.append((center, ctx))\n",
    "        else:  # cbow\n",
    "            pairs.append((context, center))\n",
    "    return pairs\n",
    "\n",
    "# Generar ambos tipos \n",
    "skip_pairs = generate_pairs(tokens, word2idx, mode='skipgram')\n",
    "cbow_pairs = generate_pairs(tokens, word2idx, mode='cbow')\n",
    "\n",
    "print(\"Skip pairs:\", len(skip_pairs))\n",
    "print(\"CBOW pairs:\", len(cbow_pairs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34c97d7",
   "metadata": {},
   "source": [
    "### Dataset + DataLoader en PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d56f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.pairs[idx]\n",
    "        return torch.long_tensor(center), torch.long_tensor(context)\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        context_list, target = self.pairs[idx]\n",
    "        return torch.tensor(context_list, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "# collate for CBOW: pad variable-length contexts and return mask or simply average in model\n",
    "def cbow_collate(batch):\n",
    "    contexts, targets = zip(*batch)\n",
    "    # contexts is list of 1d tensors with variable length\n",
    "    lengths = [len(c) for c in contexts]\n",
    "    max_len = max(lengths)\n",
    "    padded = torch.zeros(len(contexts), max_len, dtype=torch.long)\n",
    "    mask = torch.zeros(len(contexts), max_len, dtype=torch.float)\n",
    "    for i, c in enumerate(contexts):\n",
    "        padded[i, :len(c)] = c\n",
    "        mask[i, :len(c)] = 1.0\n",
    "    targets = torch.stack(targets)\n",
    "    return padded, mask, targets\n",
    "\n",
    "\n",
    "\n",
    "# Example DataLoader\n",
    "skip_ds = SkipGramDataset(skip_pairs)\n",
    "skip_loader = DataLoader(skip_ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "cbow_ds = CBOWDataset(cbow_pairs)\n",
    "cbow_loader = DataLoader(cbow_ds, batch_size=1024, shuffle=True, collate_fn=cbow_collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21794b55",
   "metadata": {},
   "source": [
    "### Model: clase base + CBOW y Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "635dd86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecBaseModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.in_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.out_linear = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SkipGramModel(Word2VecBaseModel):\n",
    "    def forward(self, centers):\n",
    "        emb = self.in_embed(centers)       \n",
    "        logits = self.out_linear(emb)      \n",
    "        return logits\n",
    "\n",
    "class CBOWModel(Word2VecBaseModel):\n",
    "    def forward(self, contexts_padded, mask):\n",
    "        emb = self.in_embed(contexts_padded)   \n",
    "        mask = mask.unsqueeze(-1)              \n",
    "        summed = (emb * mask).sum(dim=1)       \n",
    "        lens = mask.sum(dim=1).clamp(min=1)    \n",
    "        avg = summed / lens\n",
    "        logits = self.out_linear(avg)          \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddad1e94",
   "metadata": {},
   "source": [
    "### Entrenamiento — loop único (ej: Skip-gram). Repite para CBOW cambiando loader y modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e9b4888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/102057 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'long_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Entrenar SkipGram (ejemplo)\u001b[39;00m\n\u001b[32m     42\u001b[39m skip_model = SkipGramModel(vocab_size, \u001b[32m100\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m skip_model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_cbow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Entrenar CBOW\u001b[39;00m\n\u001b[32m     46\u001b[39m cbow_model = CBOWModel(vocab_size, \u001b[32m100\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloader, epochs, lr, is_cbow)\u001b[39m\n\u001b[32m     10\u001b[39m total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     11\u001b[39m it = tqdm(dataloader, desc=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_cbow\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MineriaTextos/Labs/Word-Embeddings-from-Scratch/.venv/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MineriaTextos/Labs/Word-Embeddings-from-Scratch/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MineriaTextos/Labs/Word-Embeddings-from-Scratch/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MineriaTextos/Labs/Word-Embeddings-from-Scratch/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MineriaTextos/Labs/Word-Embeddings-from-Scratch/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mSkipGramDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m      7\u001b[39m     center, context = \u001b[38;5;28mself\u001b[39m.pairs[idx]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong_tensor\u001b[49m(center), torch.long_tensor(context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MineriaTextos/Labs/Word-Embeddings-from-Scratch/.venv/lib/python3.11/site-packages/torch/__init__.py:2757\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   2754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[32m   2755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[34m__name__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2757\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch' has no attribute 'long_tensor'"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_model(model, dataloader, epochs=5, lr=1e-3, is_cbow=False):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        it = tqdm(dataloader, desc=f'Epoch {epoch}/{epochs}')\n",
    "        for batch in it:\n",
    "            optimizer.zero_grad()\n",
    "            if is_cbow:\n",
    "                contexts_padded, mask, targets = batch\n",
    "                contexts_padded = contexts_padded.to(device).long()   # indices -> long\n",
    "                mask = mask.to(device)\n",
    "                targets = targets.to(device).long()                   # ensure long\n",
    "                logits = model(contexts_padded, mask)\n",
    "            else:\n",
    "                centers, contexts = batch  # contexts are single indices for skipgram\n",
    "                centers = centers.to(device).long()                  # indices -> long\n",
    "                targets = contexts.to(device).long()                 # ensure long\n",
    "                logits = model(centers)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "            it.set_postfix(loss=loss.item())\n",
    "        avg = total_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch {epoch} avg loss: {avg:.4f}\")\n",
    "        losses.append(avg)\n",
    "    # plot loss\n",
    "    plt.plot(losses, marker='o')\n",
    "    plt.title('Training loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Avg loss')\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "# Entrenar SkipGram (ejemplo)\n",
    "skip_model = SkipGramModel(vocab_size, 100)\n",
    "skip_model = train_model(skip_model, skip_loader, epochs=5, lr=1e-3, is_cbow=False)\n",
    "\n",
    "# Entrenar CBOW\n",
    "cbow_model = CBOWModel(vocab_size, 100)\n",
    "cbow_model = train_model(cbow_model, cbow_loader, epochs=5, lr=1e-3, is_cbow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a67f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
